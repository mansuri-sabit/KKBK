# Gemini 2.0 Flash Streaming Implementation Guide

## Overview

Your application now uses **real-time streaming** with Gemini 2.0 Flash API. This means tokens are delivered as they're generated, providing faster response times and better user experience for voice conversations.

## What Changed

### âœ… Real Streaming Implementation

The previous implementation was simulating streaming by splitting the complete response. Now it uses **Server-Sent Events (SSE)** to receive tokens in real-time from Gemini's API as they're generated.

### Key Features

1. **Real-time Token Streaming**: Tokens arrive as they're generated by Gemini
2. **Lower Latency**: Responses start appearing faster (no need to wait for complete response)
3. **Better UX**: Users see/hear responses start immediately
4. **Efficient**: Processes data stream incrementally

## How It Works

### API Endpoint

The streaming uses the `streamGenerateContent` endpoint:
```
https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent
```

### Implementation Details

1. **Fetch with Streaming**: Uses native Node.js `fetch()` API (Node.js 18+)
2. **SSE Parsing**: Processes Server-Sent Events format (`data: {...}`)
3. **Incremental Updates**: Extracts text deltas from each chunk
4. **Callback System**: Calls `onToken(token, isComplete)` for each token

### Code Flow

```javascript
// In utils/aiService.js
async generateWithGeminiStreaming(session, userText, onToken) {
  // 1. Build prompt with conversation history
  // 2. Call Gemini streaming endpoint
  // 3. Read stream chunks using ReadableStream
  // 4. Parse SSE format (data: {...})
  // 5. Extract text deltas from each chunk
  // 6. Call onToken() for each token immediately
  // 7. Return complete response when done
}
```

## Usage

### Current Usage (Already Integrated)

Your `server.js` already uses streaming:

```javascript
const onToken = async (token, isComplete) => {
  if (token && !isComplete) {
    // Token received - send to TTS immediately
    textToSpeak += token;
    // Stream TTS audio as tokens arrive
  }
};

const replyText = await aiService.generateAgentReplyStreaming(
  session, 
  transcribedText, 
  onToken
);
```

### Environment Variables

No new environment variables needed! Uses existing:
- `GEMINI_API_KEY` - Your Gemini API key (required)

Optional:
- `GEMINI_STREAM_API_URL` - Override streaming endpoint (defaults to correct URL)

## Benefits for Voice Conversations

1. **Faster Start**: TTS can start speaking as soon as first tokens arrive
2. **Lower Latency**: Reduced time-to-first-audio
3. **Natural Flow**: More conversational feel with incremental responses
4. **Efficient**: No artificial delays, real-time processing

## Monitoring

### Logs to Watch

When streaming is working, you'll see:
```
ðŸ“¡ [call_123] Using Gemini 2.0 Flash streaming API...
âœ… [call_123] Gemini streaming complete: 45 chars, 12 tokens (latency: 1250ms)
```

### Token Count

The log shows how many tokens were received, helping you monitor streaming performance.

## Troubleshooting

### No Tokens Received

If `onToken` is never called:
1. Check `GEMINI_API_KEY` is set correctly
2. Verify network connectivity to Google's API
3. Check server logs for API errors

### Slow Streaming

If tokens arrive slowly:
1. Check your network connection
2. Verify Gemini API status
3. Consider adjusting `maxOutputTokens` in generation config

### Errors

Common errors:
- `GEMINI_API_KEY not configured` â†’ Set your API key in `.env`
- `HTTP 401` â†’ Invalid API key
- `HTTP 429` â†’ Rate limit exceeded (wait and retry)
- `HTTP 500` â†’ Gemini API error (temporary, retry)

## Testing

To test streaming manually:

```javascript
import { aiService } from './utils/aiService.js';

const session = {
  callId: 'test',
  conversationHistory: []
};

const tokens = [];
await aiService.generateAgentReplyStreaming(
  session,
  'Hello, how are you?',
  (token, isComplete) => {
    if (token) tokens.push(token);
    console.log('Token:', token, 'Complete:', isComplete);
  }
);

console.log('Full response:', tokens.join(''));
```

## Next Steps

1. âœ… Streaming is now active
2. Monitor logs to see token counts and latency
3. Adjust `maxOutputTokens` if needed (currently 150 for voice)
4. Fine-tune `temperature` (currently 0.7) for desired response style

## Additional Resources

- [Gemini API Documentation](https://ai.google.dev/docs)
- [Streaming API Guide](https://ai.google.dev/gemini-api/docs/streaming)
- [Node.js Fetch API](https://nodejs.org/api/globals.html#fetch)

---

**Status**: âœ… Streaming implementation complete and active!

